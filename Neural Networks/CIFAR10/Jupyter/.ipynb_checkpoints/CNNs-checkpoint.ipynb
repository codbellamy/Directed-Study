{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs and Computer Vision\n",
    "##### By Cody Bellamy\n",
    "\n",
    "In this notebook, we will explore, break down, and analyze the structures and reasoning behind a typical CNN and the components that go into creating a CNN. This notebook assumes the reader has a entry-level understanding of artificial neural networks and related algorithms.\n",
    "\n",
    "## Prerequisites\n",
    "1. [Python](https://www.python.org/downloads/)\n",
    "2. [PyTorch](https://pytorch.org/get-started)\n",
    "\n",
    "## The Data\n",
    "It is important to understand the data that you will be working with. When working with images, typically the image is broken down into 3 dimensions: height, width, and channels. A channel can be thought of as a layer. With many layers stacked on top of each other, we can construct an image. A typical image consists of 3 channels (RGB). You can imagine that we can remove all the red values for each pixel and construct a new image consisting of only red pixels of varying brightness. We can further extract the green and blue channels from the image. For the purposes of this notebook, we will consider only .jpg images with 3 RGB channels. A CNN does not care so much for color as it does for features. In this sense, it is important to normalize our images against the mean and std across our training data. Each input is a linear combination of all channels and then a dot product with the kernels to create new images. More on this process later. For now, we need to consider that for our data, we need to normalize the channels to get more meaningful data from our inputs. For the purposes of this notebook, we will be using pre-calculated normalizations for the data.\n",
    "\n",
    "To get started, we are going to import some packages we plan on using for this CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define the transformations that will be performed on our input images. Some transformations will be to consolidate the size, other transformations will be done to the values of the pixels for normalizations. As shown below, we can simulate \"new\" data using the same images by adding 4 pixels of padding and cropping randomly in a 32x32 area. We can additionally decide to horizontally flip the image. The last step is to normalize our data with the means and stds calculated for each channel beforehand.\n",
    "\n",
    "**Note:** We are assuming all inputs have the same dimensions for height, width, and channels. If your input is of different sizes, random crop will work for traning; but, you must call transforms.resize() for your testing data before converting to a tensor.\n",
    "\n",
    "A tensor object is simply a matrix with variable number of dimensions. Tensors have what is called a \"shape\" and the shape of a tensor object can be viewed by calling the object variable \"shape\". EG: `print(myTensor.shape)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data for training and testing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Normalize the test set same as training set without augmentation\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to load our data for this example, we are going to be using the CIFAR10 dataset. PyTorch allows for your code to download and utilize the CIFAR10 dataset. Note that we are assigning the transformations we composed in the last step to the data we are loading.\n",
    "\n",
    "`num_workers` refers to how many processes/batches you are going to run in parallel. If `num_workers` is greater than 0, I recommend encapsulating all of your code in a main function and running the following code to prevent infinite recursion.\n",
    "```\n",
    "def run():\n",
    "    torch.multiprocessing.freeze_support()\n",
    "    main()\n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to define what categories we will be classifying. If you are working with your own data and these are all placed into seperate folders, you can name each class what you named the folders. Otherwise, we will use the same structure that is used in CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Structure\n",
    "\n",
    "Now that we have set up our data, we now need to define the structure of our CNN. This can be broken down into two parts. The first part are our convolutional blocks. The second part is a fully connected neural network to process the outputs of our convolutions.\n",
    "\n",
    "### Convolutional Blocks\n",
    "Convolutional blocks are a series of steps that are repeated for extracting the most feature information. The number of blocks depends on the size of the images, the features you are trying to extract, and the computational constraints of your resources. Typically, one block consists of the convolutions, normalizations, an activation, a second set of convolutions, another activation, and finally a pooling function. This structure helps to maintain a balance of retaining important feature data derived from the convolutions and reducing the number of inputs to the next layer.\n",
    "\n",
    "Let's break down a 2D convolution in terms of our CNN. The first step is to generate our kernels (filters). These are randomly generated in PyTorch. Each kernel is designed to extract some unknown features from our input channels. There are some considerations to make when determining the size of our kernels for any particular convolution. If the features we are trying to classify are sometimes near the borders of the image, it might be necessary to pad our input channels with some pixels. If the defining features are particularly large, we may want our kernels to be larger. There are many variables to consider when designing a convolutional layer. In our example, our images are particularly small with more intricate features that help to classify our images. In our example, there are particularly small features that define our images. We will set our `kernel_size=3` and `padding=1` for all layers.\n",
    "\n",
    "When a convolution is done, we will first \"stack\" the input channels on top of eachother. Next, a linear combination of each pixel is calculated using the weights of the network. This essentially will squash our 3rd dimension (channels). In doing so, the shape of our input becomes only width and height. Finally, the kernels are passed over, one at a time, the entire image performing dot products to produce the new images. The output of each convolution with each different kernel becomes a new channel. Thus, if we have 3 input channels and 32 output channels, we will need 32 kernels to create a new shape. Since our padding of 1 pixel and kernel size of 3 pixels is constant, we will generate a 31x31x32 output shape. The more kernels we use, the more features we can isolate from our image such as vertical lines, horizontal lines, changes in contrast, etc.\n",
    "\n",
    "With this information, we can construct our first step in the convoultional block.\n",
    "`nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)`\n",
    "\n",
    "We then normalize our data using all 32 output channels produced from this convolution and apply an activation. ReLU is fast, and efficient in classifier networks.\n",
    "```\n",
    "nn.BatchNorm2d(32)\n",
    "nn.ReLU(inplace=True)\n",
    "```\n",
    "**Note:** we use `inplace=True` to help preserve memory. Our input data will be lost and replaced with the output of our activation. This paramter is telling PyTorch that we do not care to save the input values.\n",
    "\n",
    "We can repeat our convolutions and another activation.\n",
    "```\n",
    "nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "nn.Relu(inplace=True)\n",
    "```\n",
    "\n",
    "And finally, we are going to pool our output from this block. Pooling is not a necessary step if you have the computational power. Although, if you have been keeping track, our current tensor shape until this point is 30x30x64. As we continue convolving, normalizing, and applying activations, the computational strain will be exponential. Thus, we are going to pool the data we have. If we perform max pooling, we will take the max value from a section and this becomes the new value of a new image. With max pooling, we have two parameters that we can control. `kernel_size` and `stride`. A 2x2 kernel with a stride of 2 will take the max of a 2x2 grid, then stride 2 pixels to the next adjacent 2x2 grid and take the max of those. We can reduce the image size by a factor of 4. The output shape of our max pooling will result in 15x15x64. This is much more manageable and retains most of the important data. So our code for the block becomes:\n",
    "`nn.MaxPool2d(kernel_size=2, stride=2)`\n",
    "\n",
    "For CIFAR10, we are going to repeat these blocks twice more for 3 total convolutional blocks. The code below is a snippet from the network class (which we will construct momentarily).\n",
    "```\n",
    "# Conv Layer block 1\n",
    "nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "nn.BatchNorm2d(32),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "# Conv Layer block 2\n",
    "nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "nn.BatchNorm2d(128),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "nn.Dropout2d(p=0.05),\n",
    "\n",
    "# Conv Layer block 3\n",
    "nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "nn.BatchNorm2d(256),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "```\n",
    "\n",
    "### The FC Network\n",
    "After our convolutions have been completed, we will squash our shape into one dimension (4096x1x1) and feed it into a fully connected artificial neural network (FC-ANN). This portion should be more or less self explanitory. Our FC-ANN must have enough layers to properly process the inputs and must have 10 outputs with a softmax activation to determine the network's \"confidence\" in its classification.\n",
    "```\n",
    "nn.Dropout(p=0.1),\n",
    "nn.Linear(4096, 1024),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.Linear(1024, 512),\n",
    "nn.ReLU(inplace=True),\n",
    "nn.Dropout(p=0.1),\n",
    "nn.Linear(512, 10)\n",
    "```\n",
    "\n",
    "### Putting it All Together\n",
    "Finally, with this information and structure, we can design our CNN class. The `forward` function will be called by PyTorch. This function essentially defines what a forward pass looks like for our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CNN and Linear NN structure\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=0.05),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        \n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we've built our network, it is important to train it. Similar to a FC-ANN, we will take batches from our training set and do forward passes. We can asses the performance of our network against the labels and the perform back propagation. Luckily, PyTorch does a lot of the heavy lifting and optimizations to speed this up. So let's create our network and define our loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create an instance of our network\n",
    "net = Net()\n",
    "\n",
    "# Optimize and define parameters of our network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Network States\n",
    "Before training our network, we are going to define a location and filenames to save our network after training each epoch. Saving our network state allows us to load our trained network in the future to the exact same state (weights and biases) when we want to evaluate inputs later.\n",
    "\n",
    "**Note: *YOU MUST*** create the `states` folder in the local directory **FIRST**. This script **DOES NOT** create the folder for you. It would not be fun to train an entire epoch first and then experience a runtime error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './states/cifar_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done all the setup, it's time to train our network. We want to train our network for only a certain number of epochs. For each epoch, we want to forward pass our batches, back propagate each mini batch, and save our network state at the end of the epoch. Using this information, our training loop looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train(True) # Set our network to training mode\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Save the state of our network\n",
    "    torch.save(net.state_dict(), PATH+str(epoch+1))\n",
    "    \n",
    "    # Test the network after each epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained Networks\n",
    "After successfully training your network, it may become useful to load the state of your network again. In this example, we have trained a network to a high degree of success. The next step is to package our network so that it may be used again using relatively low resource cost. For our example, we will be loading a state of our network that has had 62 epochs of training and reaching diminishing returns on accuracy improvement. The overall accuracy of the network has been calculated to 92% against the test batch.\n",
    "\n",
    "### Prerequisites\n",
    "Below are the files that are required for this section. Place them in the same directory as this notebook.\n",
    "- [LayeredCNN.py](https://github.com/codbellamy/Directed-Study/blob/master/Neural%20Networks/CIFAR10/LayeredCNN.py)\n",
    "- [State File](https://github.com/codbellamy/Directed-Study/blob/master/Neural%20Networks/CIFAR10/states/3xConv_MaxPool/cifar_net_62)\n",
    "- An image of your choice from any source (extension cannot be .png)\n",
    "\n",
    "### Steps\n",
    "Following these simple steps will demonstrate how to use a pretrained network as a function.\n",
    "1. Initial setup\n",
    "2. Load the network\n",
    "3. Pass the image through\n",
    "4. Interpret the output\n",
    "\n",
    "There will be some additional code added for some cool visualizations which will take more processing time and resources. For the purposes of functionality, these steps can be skipped. They will be noted as an optional step.\n",
    "\n",
    "#### Step 1: Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from LayeredCNN import Net\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# If you are planning on plotting the convolution outputs\n",
    "from math import sqrt\n",
    "from math import ceil\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# Trained state\n",
    "STATE_FILE = './cifar_net_62'\n",
    "\n",
    "# Input image\n",
    "IMAGE = 'image.jpg'\n",
    "\n",
    "# Outputs\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this point, we need to set up the transformations that will be done to our input images. There are a few key concepts to keep in mind while doing this. Recall that when we trained the network, the tensor shape was 32x32x3. It is crucial that we match this shape **EXACTLY**. If the shape is not the same, there will be a mismatch in our code for the required number of inputs to the actual number of inputs. Additionally, our training and testing data has had its values normalized. Thus, we must also normalize our input data. Luckily, PyTorch proviedes the resources to make these steps easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations and data normalizations to adjust input similar to training data\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Registering Forward Hooks\n",
    "The following step is registering forward hooks. Essentially, this is letting PyTorch know that we want to save some values in memory during the forward pass. In our case, we are planning on converting the data between two layers into images. The following function will allow us to create a dictionary of forward hooks called `activation` that we can use to print each activation as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of activations for visualizations\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to preload our image into memory using our transformations. This step is simply to get everything ready to pass through our network once it is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Reading image...')\n",
    "if IMAGE[-3:] == 'png':\n",
    "    raise Exception('Extension cannot be png')\n",
    "image = Image.open(IMAGE) # Image to pass through the network\n",
    "image_t = data_transforms(image) # Perform transformations to the image\n",
    "batch_t = torch.unsqueeze(image_t, 0) # Convert the matrix to a vector\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Load the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Initializing network...')\n",
    "net = Net() # Create the network, this must be the exact same structure as the trained network\n",
    "net.load_state_dict(torch.load(STATE_FILE)) # Load the trained state from file\n",
    "net.eval() # Set the network to evaluation mode\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Register Forward Hooks\n",
    "Now that our network has been initialized, we are going to register the hooks where we want to monitor the data. I have chosen the indexes within `self.conv_layer` where I want to register a hook at the output. Each index value corresponds to each layer. 17 was chosen because it is the output of the final max pooling step prior to feeding into the FC-ANN. I belive it would be interesting to see a visualization of the data after all convolutions have finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register forward pass hooks for each convolutional layer in the sequence\n",
    "CONV_LAYERS = (0,3,7,10,14,17)\n",
    "for layer in CONV_LAYERS:\n",
    "    name = 'conv_layer'+str(layer)\n",
    "    net.conv_layer[layer].register_forward_hook(get_activation(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
